{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Perform tokenization, Stemming,lemmatization**"
      ],
      "metadata": {
        "id": "zF90PRNUaTU8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eilgc1HwY4Yo",
        "outputId": "75bfce93-3dd8-4a67-c20a-7756902dc72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mqAfBcWbY5Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Importing the Class-based tokenizers for better compatibility\n",
        "from nltk.tokenize import (\n",
        "    word_tokenize,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer,\n",
        "    WhitespaceTokenizer  # Use the class instead of the function\n",
        ")\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab') # Required for newer NLTK versions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I83L2m0_ZyQc",
        "outputId": "0b6a8b4d-4e5e-40cf-8bd9-3dbd7765a087"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**"
      ],
      "metadata": {
        "id": "kll4MgoZapWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs' heads. #NLP is cool! :)\"\n",
        "\n",
        "# --- TOKENIZATION ---\n",
        "print(\"--- Tokenization ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tOmP_TgZ62-",
        "outputId": "19e40b69-fbd0-44f2-883a-23565517ed91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tokenization ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Whitespace** **Tokenization**"
      ],
      "metadata": {
        "id": "435E4wcuavdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Whitespace Tokenization (Class-based)\n",
        "ws_tokenizer = WhitespaceTokenizer()\n",
        "print(\"Whitespace:\", ws_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWLpJQ__Z_RV",
        "outputId": "16aa75ca-98cd-462c-d342-064de34c34ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', \"dogs'\", 'heads.', '#NLP', 'is', 'cool!', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Punctuation-based (Standard word_tokenize)\n",
        "print(\"Punctuation-based:\", word_tokenize(text))\n",
        "\n",
        "# 3. Treebank Tokenizer\n",
        "tb_tokenizer = TreebankWordTokenizer()\n",
        "print(\"Treebank:\", tb_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU1MoRI4aBGw",
        "outputId": "df6aac07-2bd9-4743-adb0-19efe483cca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', \"'\", 'heads', '.', '#', 'NLP', 'is', 'cool', '!', ':', ')']\n",
            "Treebank: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', \"'\", 'heads.', '#', 'NLP', 'is', 'cool', '!', ':', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tweet Tokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(\"Tweet:\", tweet_tokenizer.tokenize(text))\n",
        "\n",
        "# 5. MWE (Multi-Word Expression) Tokenizer\n",
        "mwe_text = \"The United States of America is a large country.\"\n",
        "mwe_tokenizer = MWETokenizer([('United', 'States', 'of', 'America')])\n",
        "# Note: MWE tokenizer requires a list of already split tokens\n",
        "print(\"MWE:\", mwe_tokenizer.tokenize(word_tokenize(mwe_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWUPbyFNaGEx",
        "outputId": "1f9cf9da-4cb6-4275-8562-e4d0cb90a288"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', \"'\", 'heads', '.', '#NLP', 'is', 'cool', '!', ':)']\n",
            "MWE: ['The', 'United_States_of_America', 'is', 'a', 'large', 'country', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** STEMMING & LEMMATIZATION **"
      ],
      "metadata": {
        "id": "mKtTScOha1_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEMMING & LEMMATIZATION ---\n",
        "words = [\"jumping\", \"jumps\", \"jumped\", \"foxes\", \"better\"]\n",
        "\n",
        "print(\"\\n--- Stemming ---\")\n",
        "ps = PorterStemmer()\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print(\"Porter:\", [ps.stem(w) for w in words])\n",
        "print(\"Snowball:\", [ss.stem(w) for w in words])\n",
        "\n",
        "print(\"\\n--- Lemmatization ---\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemmatizer:\", [lemmatizer.lemmatize(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2-y4a9JaMEZ",
        "outputId": "29d29d54-41f4-407c-8cd0-852e6d691192"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Stemming ---\n",
            "Porter: ['jump', 'jump', 'jump', 'fox', 'better']\n",
            "Snowball: ['jump', 'jump', 'jump', 'fox', 'better']\n",
            "\n",
            "--- Lemmatization ---\n",
            "Lemmatizer: ['jumping', 'jump', 'jumped', 'fox', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "migEkeu0aSBT"
      }
    }
  ]
}