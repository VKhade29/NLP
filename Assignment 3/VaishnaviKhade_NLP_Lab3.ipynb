{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "SYGafVPRgcsv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Updated Downloads for the latest NLTK version\n",
        "resources = [\n",
        "    'stopwords',\n",
        "    'wordnet',\n",
        "    'omw-1.4',\n",
        "    'punkt',\n",
        "    'punkt_tab',\n",
        "    'averaged_perceptron_tagger_eng'  # The specific fix for your error\n",
        "]\n",
        "\n",
        "for res in resources:\n",
        "    nltk.download(res)\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # Using Bi-grams to capture phrases like \"Quantum computing\"\n",
        "        self.tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
        "\n",
        "    def _get_wordnet_pos(self, word):\n",
        "        \"\"\"Map POS tag to format WordNetLemmatizer accepts\"\"\"\n",
        "        # We use the 'eng' version explicitly if needed,\n",
        "        # but nltk.pos_tag usually handles the mapping internally once downloaded\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    def clean(self, text):\n",
        "        # Remove URLs and noise\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # POS-Aware Lemmatization\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        cleaned_tokens = [\n",
        "            self.lemmatizer.lemmatize(w, self._get_wordnet_pos(w))\n",
        "            for w in tokens if w not in self.stop_words\n",
        "        ]\n",
        "        return \" \".join(cleaned_tokens)\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "raw_data = {\n",
        "    'text': [\n",
        "        \"The economy is fluctuating significantly in 2024!\",\n",
        "        \"Biological researchers are studying the feline genome.\",\n",
        "        \"Quantum computing remains a theoretical frontier for scientists.\",\n",
        "        \"Domestic cats exhibit predatory instincts even when fed.\"\n",
        "    ],\n",
        "    'target': ['finance', 'science', 'science', 'nature']\n",
        "}\n",
        "df = pd.DataFrame(raw_data)\n",
        "\n",
        "processor = TextProcessor()\n",
        "print(\"Starting POS-Aware Lemmatization (High Precision)...\")\n",
        "df['refined_text'] = df['text'].apply(processor.clean)\n",
        "\n",
        "# Label Encoding\n",
        "df['encoded_label'] = processor.label_encoder.fit_transform(df['target'])\n",
        "\n",
        "# TF-IDF Representation\n",
        "tfidf_matrix = processor.tfidf.fit_transform(df['refined_text'])\n",
        "\n",
        "# Saving Outputs\n",
        "joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')\n",
        "df.to_csv('final_processed_data.csv', index=False)\n",
        "\n",
        "print(\"\\nSuccess! Sample of cleaned text:\")\n",
        "print(df[['text', 'refined_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZvSHlNshA0s",
        "outputId": "34a2819c-5e61-4908-bc52-77da2ad687cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting POS-Aware Lemmatization (High Precision)...\n",
            "\n",
            "Success! Sample of cleaned text:\n",
            "                                                text  \\\n",
            "0  The economy is fluctuating significantly in 2024!   \n",
            "1  Biological researchers are studying the feline...   \n",
            "2  Quantum computing remains a theoretical fronti...   \n",
            "3  Domestic cats exhibit predatory instincts even...   \n",
            "\n",
            "                                        refined_text  \n",
            "0                    economy fluctuate significantly  \n",
            "1          biological researcher study feline genome  \n",
            "2  quantum compute remains theoretical frontier s...  \n",
            "3   domestic cat exhibit predatory instinct even fed  \n"
          ]
        }
      ]
    }
  ]
}